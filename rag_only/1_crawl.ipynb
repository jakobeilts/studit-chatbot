{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-24T15:56:20.622630Z",
     "start_time": "2025-07-24T15:56:20.420802Z"
    }
   },
   "source": [
    "import time, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "import gzip, pickle\n",
    "from helper.list_of_all_html import urls"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Crawling\n",
    "Since we are using data from websites we are building a simple webcrawler and store the data in the *docs* variable"
   ],
   "id": "14978d38e08e5645"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:56:20.759671Z",
     "start_time": "2025-07-24T15:56:20.754622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_visible_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    return \"\\n\".join(line.strip()\n",
    "                     for line in soup.get_text(\"\\n\").splitlines()\n",
    "                     if line.strip())\n",
    "\n",
    "def crawl_urls(urls, delay=0.4) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(url,\n",
    "                             headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "                             timeout=15)\n",
    "            r.raise_for_status()\n",
    "            text = extract_visible_text(r.text)\n",
    "            docs.append(Document(page_content=text,\n",
    "                                 metadata={\"url\": url}))\n",
    "        except Exception as exc:\n",
    "            print(f\"[!!] {url}: {exc}\")\n",
    "        time.sleep(delay)\n",
    "    return docs"
   ],
   "id": "1bf17c34e7889ffd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are saving the docs in a pickle file to use in another jupyter notebook",
   "id": "bb1671bdbc7b483c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:58:55.067568Z",
     "start_time": "2025-07-24T15:56:20.777442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- save ---\n",
    "docs = crawl_urls(urls)\n",
    "with gzip.open(\"docs.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(docs, f)"
   ],
   "id": "2cf69fffd152e246",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
