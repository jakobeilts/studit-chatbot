{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`https://python.langchain.com/docs/tutorials/rag/`",
   "id": "1f00b11ec1fe5949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T16:06:27.309116Z",
     "start_time": "2025-07-24T16:06:26.875046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document   # whichever Document import you use\n",
    "import gzip, pickle\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from helper.academicCloudEmbeddings import AcademicCloudEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import streamlit as st"
   ],
   "id": "bcb919f176032563",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Indexing\n",
    "## 1. Load the data\n",
    "*In our case the data needs to be crawled first. See `crawl.ipynb`. There we are storing the documents in a pickle file we can load here*"
   ],
   "id": "94898054eb166bcd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-24T16:06:27.451919Z",
     "start_time": "2025-07-24T16:06:27.447145Z"
    }
   },
   "source": [
    "with gzip.open(\"docs.pkl.gz\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Split the loaded data\n",
    "We are splitting large documents into smaller chunks for indexing the data and passing it into a model. Large chunks would be worse for search"
   ],
   "id": "4a88ca06244ef53e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T16:06:27.486581Z",
     "start_time": "2025-07-24T16:06:27.464733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# splitten â€“ jede URL bleibt als metadata erhalten\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(docs)"
   ],
   "id": "3c28c248cfc5bf4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Store\n",
    "We are storing the created chunks:\n",
    "1. creating embeddings using the GWDG model\n",
    "2. storing in a FAISS store which can be saved locally to use later. That way we don't need to create the store every time we want to start the app"
   ],
   "id": "7f3e1af719d84598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T16:09:20.342306Z",
     "start_time": "2025-07-24T16:06:27.500497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embeddings und FAISS\n",
    "embedder = AcademicCloudEmbeddings(\n",
    "    api_key=st.secrets[\"GWDG_API_KEY\"],\n",
    "    url=st.secrets[\"BASE_URL_EMBEDDINGS\"],\n",
    ")\n",
    "store = FAISS.from_documents(chunks, embedder)\n",
    "store.save_local(\"faiss_wiki_index\")"
   ],
   "id": "3442387cf7215906",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
